{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac798da1",
   "metadata": {},
   "source": [
    "Sequence Models / NLP\n",
    "AmaÃ§\n",
    "\n",
    "Metin tabanlÄ± modelleri (LSTM, GRU, Transformers) uygulayÄ±p sonuÃ§larÄ±nÄ± kÄ±yaslamak.\n",
    "\n",
    "GÃ¶rev AdÄ±mlarÄ±\n",
    "\n",
    "Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "Kaggleâ€™dan IMDB Sentiment Analysis datasetâ€™ini indir.\n",
    "\n",
    "Train/test split hazÄ±r (varsa tekrar dÃ¼zenle).\n",
    "\n",
    "Tokenization ve Padding uygula.\n",
    "\n",
    "Model 1 â€“ LSTM / GRU\n",
    "\n",
    "Embedding Layer (Ã¶rn. 100 boyutlu).\n",
    "\n",
    "LSTM veya GRU katmanÄ± ekle.\n",
    "\n",
    "Ã‡Ä±kÄ±ÅŸ: Sigmoid (binary classification).\n",
    "\n",
    "Model 2 â€“ Transformer TabanlÄ±\n",
    "\n",
    "HuggingFaceâ€™den BERT veya RoBERTa pre-trained modeli al.\n",
    "\n",
    "LoRA veya PEFT ile fine-tune et.\n",
    "\n",
    "DeÄŸerlendirme\n",
    "\n",
    "Accuracy, Precision, Recall, F1 Score hesapla.\n",
    "\n",
    "BLEU Score (opsiyonel, sequence generation senaryosu iÃ§in).\n",
    "\n",
    "Modelleri yan yana kÄ±yasla.\n",
    "\n",
    "ðŸ“Œ Beklenen Ã‡Ä±ktÄ±lar: F1 Score tablosu, confusion matrix, model karÅŸÄ±laÅŸtÄ±rma raporu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a98ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09ff40",
   "metadata": {},
   "source": [
    "## Veri HazÄ±rlÄ±ÄŸÄ±\n",
    "- Veri seti text ve label olarak sÃ¼tunlara ayrÄ±ldÄ± \n",
    "- \"train_test_split\" ile %80 train %20 test olarak bÃ¶lÃ¼ndÃ¼.\n",
    "- stratify = y ile train ve test setlerinde sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± orjinal veri ile aynÄ± tutuldu yani label 1 - 0 sayÄ±larÄ± dengeli\n",
    "- random_state = 42 ile split iÅŸlemi tekrarlanabilir yani her Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda aynÄ± train/test bÃ¶lÃ¼nmesi elde edilmiÅŸ olur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80f5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/movie.csv\")\n",
    "X = df[\"text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, stratify = y, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c3dd8",
   "metadata": {},
   "source": [
    "**HuggingFace + PyTorch**\n",
    "- Pre-trained tokenizer ile metinler token ID'lerine Ã§evrildi,\n",
    "- Padding ve truncation ile tÃ¼m diziler aynÄ± uzunlukta getirildi,\n",
    "- Labels tensor formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼,\n",
    "- DataLoader iÃ§in hazÄ±r hale getirildi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6432a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ufuk_delice\\Task\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e0c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = tokenizer(list(X_train), padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(X_test), padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "y_train_tensor = torch.tensor(list(y_train.values))\n",
    "y_test_tensor = torch.tensor(list(y_test.values))\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader #https://docs.pytorch.org/docs/stable/data.html#module-torch.utils.data\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea06b5",
   "metadata": {},
   "source": [
    "### GRU Model \n",
    "https://medium.com/@anishnama20/understanding-gated-recurrent-unit-gru-in-deep-learning-2e54923f3e2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bb61a",
   "metadata": {},
   "source": [
    "#### Model TanÄ±mÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b6d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size=1, num_layers=1, dropout=0.2):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, h_n = self.gru(embedded)\n",
    "        #out[:, -1, :] -> son time step'i al\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5127e",
   "metadata": {},
   "source": [
    "#### Loss ve Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31fcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUClassifier(vocab_size = len(tokenizer), embedding_dim = 100, hidden_size = 128)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e687633",
   "metadata": {},
   "source": [
    "#### Dataset ve DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfc2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], y_train_tensor)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], y_test_tensor)\n",
    "\n",
    "traind_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db007af",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de0daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6937\n",
      "Epoch 2, Loss: 0.5678\n",
      "Epoch 3, Loss: 0.3130\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965238c",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbd4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            preds.extend((outputs.squeeze() > 0.5).int().tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    prec = precision_score(true_labels, preds)\n",
    "    rec = recall_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    return acc, prec, rec, f1, cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa762b8",
   "metadata": {},
   "source": [
    "#### Test Dataset ve DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49f8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.869625\n",
      "Precision: 0.8733249051833123\n",
      "Recall: 0.8643643643643644\n",
      "F1 Score: 0.868821531882782\n",
      "Confusion Matrix:\n",
      " [[3503  501]\n",
      " [ 542 3454]]\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, confusion = evaluate(model, test_loader)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7beafd",
   "metadata": {},
   "source": [
    "### LSTM Model\n",
    "https://medium.com/@techwithjulles/recurrent-neural-networks-rnns-and-long-short-term-memory-lstm-creating-an-lstm-model-in-13c88b7736e2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549aa28a",
   "metadata": {},
   "source": [
    "#### Model TanÄ±mÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5a67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size=1, num_layers=1, dropout=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(embedded)\n",
    "        out = self.fc(out[:, -1, :])  # son time stepâ€™i al\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3297af",
   "metadata": {},
   "source": [
    "#### Loss ve Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027f907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(vocab_size=len(tokenizer), embedding_dim=100, hidden_size=128)\n",
    "criterion = nn.BCELoss()  # veya BCEWithLogitsLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68018f3f",
   "metadata": {},
   "source": [
    "#### Dataset ve DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c2fb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], y_train_tensor)\n",
    "test_dataset  = TensorDataset(test_encodings['input_ids'], y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf4948",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "893894eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6930\n",
      "Epoch 2, Loss: 0.6617\n",
      "Epoch 3, Loss: 0.5753\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a372d",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10f7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            preds.extend((outputs.squeeze() > 0.5).int().tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "    return preds, true_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c523c",
   "metadata": {},
   "source": [
    "#### Test Dataset ve DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc5e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Precision: 0.7698042870456664\n",
      "Recall: 0.8268268268268268\n",
      "F1 Score: 0.7972972972972973\n",
      "Confusion Matrix:\n",
      " [[3016  988]\n",
      " [ 692 3304]]\n"
     ]
    }
   ],
   "source": [
    "preds, true_labels = evaluate(model, test_loader)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, preds))\n",
    "print(\"Precision:\", precision_score(true_labels, preds))\n",
    "print(\"Recall:\", recall_score(true_labels, preds))\n",
    "print(\"F1 Score:\", f1_score(true_labels, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(true_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a9c0b",
   "metadata": {},
   "source": [
    "## GRU & LSTM KarÅŸÄ±laÅŸtÄ±rmasÄ±\n",
    "\n",
    "| Metric           | GRU                        | LSTM                       |\n",
    "| ---------------- | -------------------------- | -------------------------- |\n",
    "| Accuracy         | 0.8696                     | 0.79                       |\n",
    "| Precision        | 0.8733                     | 0.7698                     |\n",
    "| Recall           | 0.8644                     | 0.8268                     |\n",
    "| F1 Score         | 0.8688                     | 0.7973                     |\n",
    "| Confusion Matrix | [[3503, 501], [542, 3454]] | [[3016, 988], [692, 3304]] |\n",
    "\n",
    "\n",
    "\n",
    "1. GRU daha yÃ¼ksek Accuracy ve F1 Score verdi\n",
    "    - Bu veri seti (IMDB, yorumlar kÄ±sa/orta uzunlukta) GRU iÃ§in daha uygun olabilir.\n",
    "2. LSTM Recall biraz yÃ¼ksek ama Precision dÃ¼ÅŸÃ¼k\n",
    "    - Yani LSTM pozitifleri daha fazla yakalÄ±yor ama yanlÄ±ÅŸ pozitif tahminleri de artÄ±rÄ±yor.\n",
    "3. Confusion Matrix farkÄ±\n",
    "    - LSTMâ€™de FP = 988 â†’ GRUâ€™ya gÃ¶re neredeyse iki kat\n",
    "    - FN = 692 â†’ LSTM biraz daha fazla kaÃ§Ä±rÄ±yor\n",
    "4. Genel Yorum\n",
    "    - Bu veri setinde GRU daha dengeli ve baÅŸarÄ±lÄ± Ã§Ä±ktÄ±.\n",
    "    - LSTMâ€™in avantajÄ± genellikle Ã§ok uzun sekanslar ve daha karmaÅŸÄ±k baÄŸÄ±mlÄ±lÄ±klar iÃ§in ortaya Ã§Ä±kar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852acba",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5711ef",
   "metadata": {},
   "source": [
    "#### HuggingFace Tokenizer\n",
    "https://medium.com/data-science-collective/how-to-fine-tune-an-llm-on-your-data-with-hugging-face-lora-563e57ca8c1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39e5381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3150c",
   "metadata": {},
   "source": [
    "Pre-trained BERT modeli ve tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c72a017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9965ae",
   "metadata": {},
   "source": [
    "LoRA KonfigÃ¼rasyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2445ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                  # low-rank boyutu\n",
    "    lora_alpha=32,        # Ã¶lÃ§ekleme faktÃ¶rÃ¼\n",
    "    target_modules=[\"query\", \"value\"],  # hangi aÄŸÄ±rlÄ±klara LoRA uygulanacak\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941d1af",
   "metadata": {},
   "source": [
    "Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09aaac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, y_train.values)\n",
    "test_dataset  = IMDbDataset(test_encodings, y_test.values)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=8, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b532aa9",
   "metadata": {},
   "source": [
    "Training Loop / Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "686703c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca661ad",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d40326",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1  # CPU iÃ§in baÅŸlangÄ±Ã§ta 1 epoch yeterli\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Batch'i cihaza taÅŸÄ±\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass ve optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92827a7f",
   "metadata": {},
   "source": [
    "Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c365d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred_labels = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            preds.extend(pred_labels.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    prec = precision_score(true_labels, preds)\n",
    "    rec = recall_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    return acc, prec, rec, f1, cm\n",
    "\n",
    "accuracy, precision, recall, f1, confusion = evaluate(model, test_loader)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
